{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import boto3\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinIO configuration\n",
    "minio_url = \"http://localhost:9000\"\n",
    "access_key = \"admin\"\n",
    "secret_key = \"admin123\"\n",
    "bucket_name = \"processed-reports\"\n",
    "\n",
    "# Set up MinIO client\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=minio_url,\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the 'processed-reports' bucket\n",
    "def list_processed_files(bucket_name):\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "    files = [content['Key'] for content in response.get('Contents', [])]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a file from MinIO\n",
    "def download_file(file_name, local_path):\n",
    "    s3_client.download_file(bucket_name, file_name, local_path)\n",
    "    print(f\"Downloaded {file_name} to {local_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in processed-reports bucket: ['processed_A-Study-on-Consumer-Brand-Awareness-of-Fast-Moving-Consumer-Goods.pdf.txt']\n",
      "Downloaded processed_A-Study-on-Consumer-Brand-Awareness-of-Fast-Moving-Consumer-Goods.pdf.txt to Data_dump/processed_A-Study-on-Consumer-Brand-Awareness-of-Fast-Moving-Consumer-Goods.pdf.txt\n"
     ]
    }
   ],
   "source": [
    "# List all files in the processed-reports bucket\n",
    "files = list_processed_files(bucket_name)\n",
    "print(f\"Files in processed-reports bucket: {files}\")\n",
    "\n",
    "# Example: Download the first file to local storage\n",
    "if files:\n",
    "    download_file(files[0], f\"Data_dump/{files[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshumankumar/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained Hugging Face model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def generate_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Generate embeddings for each downloaded file\n",
    "def generate_embeddings_from_files(file_list):\n",
    "    embeddings = []\n",
    "    for file_path in file_list:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            text = f.read()\n",
    "            embedding = generate_embedding(text)\n",
    "            embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "# Example: Generate embeddings for downloaded files\n",
    "downloaded_files = [f\"Data_dump/{file}\" for file in files]\n",
    "document_embeddings = generate_embeddings_from_files(downloaded_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents indexed: 1\n"
     ]
    }
   ],
   "source": [
    "# Define the dimensionality of the embeddings\n",
    "dimension = 768  # This should match the embedding size\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 similarity\n",
    "\n",
    "# Add document embeddings to FAISS\n",
    "def add_to_faiss_index(embeddings):\n",
    "    embeddings_np = np.array(embeddings)  # Convert to NumPy array\n",
    "    index.add(embeddings_np)  # Add embeddings to the index\n",
    "    print(f\"Total documents indexed: {index.ntotal}\")\n",
    "\n",
    "# Index the newly generated embeddings\n",
    "add_to_faiss_index(document_embeddings)\n",
    "\n",
    "# Save the FAISS index to disk for later use\n",
    "faiss.write_index(index, \"faiss_index.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FAISS index from disk\n",
    "index = faiss.read_index(\"faiss_index.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 similar documents: [[ 0 -1 -1 -1 -1]]\n",
      "Distances: [[6.0458282e+01 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38]]\n"
     ]
    }
   ],
   "source": [
    "def retrieve_similar_documents(query, k=5):\n",
    "    query_embedding = generate_embedding(query).reshape(1, -1)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    return distances, indices\n",
    "\n",
    "# Example: Query and retrieve similar documents\n",
    "query = \"What is the effectiveness of the recent marketing campaign?\"\n",
    "distances, indices = retrieve_similar_documents(query)\n",
    "print(f\"Top {len(indices[0])} similar documents: {indices}\")\n",
    "print(f\"Distances: {distances}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Document: processed_report1.txt\n",
      "No more similar documents found.\n",
      "No more similar documents found.\n",
      "No more similar documents found.\n",
      "No more similar documents found.\n"
     ]
    }
   ],
   "source": [
    "document_mapping = {0: \"processed_report1.txt\", 1: \"processed_report2.txt\"}\n",
    "\n",
    "# Loop through the indices and check for valid ones\n",
    "for i in indices[0]:\n",
    "    if i == -1:\n",
    "        print(\"No more similar documents found.\")\n",
    "    else:\n",
    "        print(f\"Similar Document: {document_mapping.get(i, 'Document not found')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augment the Dataset with FAISS Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve similar documents and augment the dataset\n",
    "def augment_training_data_with_embeddings(queries, k=5):\n",
    "    augmented_data = []\n",
    "    for query in queries:\n",
    "        retrieved_docs = retrieve_similar_documents(query, k)\n",
    "        augmented_data.append({\n",
    "            'query': query, \n",
    "            'retrieved_docs': retrieved_docs\n",
    "        })\n",
    "    return augmented_data\n",
    "\n",
    "# Example queries for training\n",
    "queries = [\"What is the effectiveness of the recent campaign?\", \"How did the campaign perform in Q2?\"]\n",
    "\n",
    "# Augment the dataset\n",
    "augmented_dataset = augment_training_data_with_embeddings(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['query', 'retrieved_docs'],\n",
      "    num_rows: 2\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Extract query and retrieved document embeddings from augmented_dataset\n",
    "queries = [item['query'] for item in augmented_dataset]\n",
    "\n",
    "# Extract embeddings from the first part of 'retrieved_docs' tuple (ignore indices)\n",
    "retrieved_embeddings = [item['retrieved_docs'][0][0] for item in augmented_dataset]  # Only embeddings\n",
    "\n",
    "# Create a dictionary suitable for Dataset.from_dict\n",
    "dataset_dict = {\n",
    "    'query': queries,\n",
    "    'retrieved_docs': retrieved_embeddings\n",
    "}\n",
    "\n",
    "# Now create the Dataset from the dict\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# Print to confirm\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple M1 GPU (MPS)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec68ce4fbfe470795eda1c3a1a4ae4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['query', 'retrieved_docs', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Check if MPS (Apple GPU) is available and use it, otherwise fall back to CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Print the device being used (MPS or CPU)\n",
    "if device.type == \"mps\":\n",
    "    print(\"Using Apple M1 GPU (MPS)\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    \n",
    "# Disable tokenizer parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "# Tokenize the queries (BERT input)\n",
    "def tokenize_query(examples):\n",
    "    return tokenizer(examples['query'], padding='max_length', truncation=True)\n",
    "\n",
    "# Tokenize queries (MLM task, no need for FAISS embeddings)\n",
    "tokenized_query_dataset = dataset.map(tokenize_query, batched=True)\n",
    "\n",
    "# Ensure the dataset contains 'input_ids' and 'attention_mask'\n",
    "print(tokenized_query_dataset)\n",
    "\n",
    "# Set format to torch to ensure the correct format for PyTorch models\n",
    "tokenized_query_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune the Model Using Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple M1 GPU (MPS)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshumankumar/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e60eba25dcd408c8ddb6c6ad5d4481e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d08d28cdf446db9a565bc6afe3b8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c2e5a1116b442c9452a6340ee18fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.02111385017633438, 'eval_runtime': 0.1114, 'eval_samples_per_second': 17.957, 'eval_steps_per_second': 8.978, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a5a85fe1d64b8d969d2d62c70c6fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00785028375685215, 'eval_runtime': 0.0825, 'eval_samples_per_second': 24.235, 'eval_steps_per_second': 12.118, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21eeca2182b4fa78a5f428fc24ff782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.007601247169077396, 'eval_runtime': 0.0887, 'eval_samples_per_second': 22.546, 'eval_steps_per_second': 11.273, 'epoch': 3.0}\n",
      "{'train_runtime': 7.6672, 'train_samples_per_second': 0.783, 'train_steps_per_second': 0.391, 'train_loss': 0.003227462371190389, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/08 11:53:19 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_mlm_model/tokenizer_config.json',\n",
       " './fine_tuned_mlm_model/special_tokens_map.json',\n",
       " './fine_tuned_mlm_model/vocab.txt',\n",
       " './fine_tuned_mlm_model/added_tokens.json',\n",
       " './fine_tuned_mlm_model/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from transformers import DistilBertForMaskedLM, AutoTokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# Disable tokenizer parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "# Check if MPS (Apple GPU) is available and use it, otherwise fall back to CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Print the device being used (MPS or CPU)\n",
    "if device.type == \"mps\":\n",
    "    print(\"Using Apple M1 GPU (MPS)\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Load a pre-trained model with Masked Language Modeling head\n",
    "model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased').to(device)\n",
    "\n",
    "# Load the tokenizer for tokenizing queries\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_query(examples):\n",
    "    return tokenizer(examples['query'], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_query_dataset = dataset.map(tokenize_query, batched=True)\n",
    "\n",
    "# Data collator for dynamic masking during training\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15  # Mask 15% of the tokens\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mlm_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    dataloader_pin_memory=False,  # Set False for MPS (pin_memory is for CUDA)\n",
    ")\n",
    "\n",
    "# Start MLflow logging\n",
    "mlflow.set_experiment(\"MLM Fine-tuning with DistilBERT\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_param(\"learning_rate\", training_args.learning_rate)\n",
    "    mlflow.log_param(\"batch_size\", training_args.per_device_train_batch_size)\n",
    "    mlflow.log_param(\"num_epochs\", training_args.num_train_epochs)\n",
    "    \n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_query_dataset,  # Tokenized dataset\n",
    "        eval_dataset=tokenized_query_dataset,   # Optionally use a validation set\n",
    "        data_collator=data_collator,            # MLM data collator\n",
    "    )\n",
    "    \n",
    "    # Train the model with MLM\n",
    "    trainer.train()\n",
    "    \n",
    "    # Safely log metrics (only log if the value is not None)\n",
    "    metrics = trainer.state.log_history[-1]  # Get the last set of logged metrics\n",
    "    \n",
    "    # Check if 'loss' and 'epoch' are present and valid before logging\n",
    "    if 'loss' in metrics and metrics['loss'] is not None:\n",
    "        mlflow.log_metric(\"train_loss\", float(metrics['loss']))\n",
    "    if 'epoch' in metrics and metrics['epoch'] is not None:\n",
    "        mlflow.log_metric(\"epoch\", float(metrics['epoch']))\n",
    "    \n",
    "    # Log the model to MLflow\n",
    "    mlflow.pytorch.log_model(model, \"masked_language_model\")\n",
    "    \n",
    "    # Log tokenizer\n",
    "    tokenizer.save_pretrained(\"./fine_tuned_mlm_model\")\n",
    "    mlflow.log_artifact(\"./fine_tuned_mlm_model/tokenizer_config.json\", \"tokenizer\")\n",
    "\n",
    "# Save locally\n",
    "model.save_pretrained(\"./fine_tuned_mlm_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_mlm_model\")  # Save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Witout ML Flow \n",
    "\n",
    "# import os\n",
    "# import torch\n",
    "# from transformers import DistilBertForMaskedLM, AutoTokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# # Disable tokenizer parallelism\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "# # Check if MPS (Apple GPU) is available and use it, otherwise fall back to CPU\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# # Print the device being used\n",
    "# if device.type == \"mps\":\n",
    "#     print(\"Using Apple M1 GPU (MPS)\")\n",
    "# else:\n",
    "#     print(\"Using CPU\")\n",
    "\n",
    "# # Load a pre-trained model with Masked Language Modeling head\n",
    "# model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased').to(device)\n",
    "\n",
    "# # Load the tokenizer for tokenizing queries\n",
    "# tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# # Tokenize the dataset\n",
    "# def tokenize_query(examples):\n",
    "#     return tokenizer(examples['query'], padding='max_length', truncation=True)\n",
    "\n",
    "# # Assuming you have a dataset to tokenize\n",
    "# tokenized_dataset = dataset.map(tokenize_query, batched=True)\n",
    "\n",
    "# # Data collator for dynamic masking during training\n",
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer=tokenizer,\n",
    "#     mlm=True,\n",
    "#     mlm_probability=0.15  # Mask 15% of the tokens\n",
    "# )\n",
    "\n",
    "# # Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./mlm_results\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_dir=\"./logs\",\n",
    "#     dataloader_pin_memory=False,  # Set False for MPS (pin_memory is for CUDA)\n",
    "# )\n",
    "\n",
    "# # Initialize the Trainer for MLM\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_dataset,  # Tokenized dataset\n",
    "#     eval_dataset=tokenized_dataset,   # Optionally use a validation set\n",
    "#     data_collator=data_collator,      # MLM data collator\n",
    "# )\n",
    "\n",
    "# # Train the model with MLM\n",
    "# trainer.train()\n",
    "\n",
    "# # After training, you can save the fine-tuned model\n",
    "# model.save_pretrained(\"./fine_tuned_mlm_model\")\n",
    "# tokenizer.save_pretrained(\"./fine_tuned_mlm_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2 embeddings\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned MLM model\n",
    "fine_tuned_model = DistilBertForMaskedLM.from_pretrained(\"./fine_tuned_mlm_model\").to(device)\n",
    "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_mlm_model\")\n",
    "\n",
    "# Now we can move the inputs to the device (MPS or CPU)\n",
    "def generate_embeddings(dataset, model):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    for batch in dataset:\n",
    "        # Ensure conversion to tensor before moving to device\n",
    "        input_ids = torch.tensor(batch['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(batch['attention_mask']).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Set output_hidden_states=True to get the hidden states\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states  # This gives you all the hidden states\n",
    "            last_hidden_state = hidden_states[-1]  # Take the last hidden state\n",
    "\n",
    "            cls_embedding = last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "            embeddings.append(cls_embedding.cpu().numpy())  # Move back to CPU for storage\n",
    "    return embeddings\n",
    "\n",
    "# Generate embeddings from the tokenized dataset\n",
    "embeddings = generate_embeddings(tokenized_query_dataset, model)\n",
    "\n",
    "# Print the number of embeddings generated\n",
    "print(f\"Generated {len(embeddings)} embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to MinIO as report_embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import boto3\n",
    "\n",
    "# MinIO configuration\n",
    "minio_url = \"http://localhost:9000\"\n",
    "access_key = \"admin\"\n",
    "secret_key = \"admin123\"\n",
    "bucket_name = \"embeddings\"\n",
    "\n",
    "# Set up MinIO client\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=minio_url,\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_key\n",
    ")\n",
    "\n",
    "# Function to save embeddings to MinIO\n",
    "def save_embeddings_to_minio(embeddings, file_name):\n",
    "    local_processed_file = f'{file_name}.npy'\n",
    "    # Save the embeddings locally as a .npy file\n",
    "    np.save(file_name, embeddings)\n",
    "    \n",
    "    # Upload the .npy file to MinIO\n",
    "    s3_client.upload_file(f\"{file_name}.npy\", bucket_name, f\"{file_name}.npy\")\n",
    "    print(f\"Embeddings saved to MinIO as {file_name}.npy\")\n",
    "    os.remove(local_processed_file)\n",
    "\n",
    "# Example: Saving embeddings\n",
    "save_embeddings_to_minio(embeddings, \"report_embeddings\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
