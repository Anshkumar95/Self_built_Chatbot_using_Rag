{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, AutoModelForMaskedLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinIO configuration\n",
    "minio_url = \"http://localhost:9000\"\n",
    "access_key = \"admin\"\n",
    "secret_key = \"admin123\"\n",
    "processed_bucket_name = \"processed-reports\"\n",
    "embedding_bucket_name = \"embeddings\"\n",
    "\n",
    "# Set up MinIO client\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=minio_url,\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings report_embeddings loaded from MinIO\n"
     ]
    }
   ],
   "source": [
    "# Function to retrieve embeddings from the embeddings bucket\n",
    "def load_embeddings_from_minio(file_name):\n",
    "    s3_client.download_file(embedding_bucket_name, f\"{file_name}.npy\", f\"{file_name}.npy\")\n",
    "    embeddings = np.load(f\"{file_name}.npy\")\n",
    "    print(f\"Embeddings {file_name} loaded from MinIO\")\n",
    "    return embeddings\n",
    "\n",
    "# Example: Loading embeddings\n",
    "loaded_embeddings = load_embeddings_from_minio(\"report_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "# Disable tokenizer parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "# Check if MPS (Apple GPU) is available and use it, otherwise fall back to CPU\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# Force all computations to use the CPU instead of MPS\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Print the device being used (MPS or CPU)\n",
    "if device.type == \"mps\":\n",
    "    print(\"Using Apple M1 GPU (MPS)\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded from the local directory.\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned MLM model and tokenizer\n",
    "fine_tuned_model = AutoModelForMaskedLM.from_pretrained(\"./fine_tuned_mlm_model\").to(device)\n",
    "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_mlm_model\")\n",
    "print(\"Model and tokenizer loaded from the local directory.\")\n",
    "\n",
    "# Function to generate query embedding using the fine-tuned model\n",
    "def generate_query_embedding(query, model, tokenizer, device):\n",
    "    tokens = tokenizer(query, return_tensors=\"pt\", padding='max_length', truncation=True)\n",
    "    input_ids = tokens['input_ids'].to(device)\n",
    "    attention_mask = tokens['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        query_embedding = last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "    \n",
    "    return query_embedding.cpu().numpy().reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform similarity search with stored embeddings\n",
    "def find_similar_embeddings(query_embedding, stored_embeddings, top_k=5):\n",
    "    if len(stored_embeddings.shape) == 3:\n",
    "        stored_embeddings = stored_embeddings.reshape(stored_embeddings.shape[0], -1)\n",
    "    \n",
    "    similarities = cosine_similarity(query_embedding, stored_embeddings)\n",
    "    top_k_indices = np.argsort(similarities[0])[::-1][:top_k]\n",
    "    return top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in processed-reports bucket: ['processed_A-Study-on-Consumer-Brand-Awareness-of-Fast-Moving-Consumer-Goods.pdf.txt']\n"
     ]
    }
   ],
   "source": [
    "# List the files in the MinIO bucket\n",
    "def list_files_in_bucket(bucket_name):\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "    if 'Contents' in response:\n",
    "        files = [content['Key'] for content in response['Contents']]\n",
    "        print(f\"Files in {bucket_name} bucket: {files}\")\n",
    "    else:\n",
    "        print(f\"No files found in bucket {bucket_name}.\")\n",
    "\n",
    "list_files_in_bucket(processed_bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshumankumar/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Map embedding indices to report files\n",
    "report_mapping = {\n",
    "    0: \"processed_A-Study-on-Consumer-Brand-Awareness-of-Fast-Moving-Consumer-Goods.pdf.txt\"\n",
    "}\n",
    "\n",
    "# Function to retrieve reports from MinIO based on the embedding indices\n",
    "def retrieve_reports_from_indices(indices):\n",
    "    retrieved_reports = []\n",
    "    for idx in indices:\n",
    "        report_file_name = report_mapping.get(idx)\n",
    "        if report_file_name:\n",
    "            local_file_path = f\"./{report_file_name}\"\n",
    "            s3_client.download_file(processed_bucket_name, report_file_name, local_file_path)\n",
    "            with open(local_file_path, \"r\") as f:\n",
    "                report_content = f.read()\n",
    "            retrieved_reports.append(report_content)\n",
    "        else:\n",
    "            retrieved_reports.append(f\"Report with index {idx} not found.\")\n",
    "    return retrieved_reports\n",
    "\n",
    "# Load the BART model and tokenizer for summarization\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshumankumar/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Function to generate query-based summary\n",
    "def generate_query_based_summary(query, report_texts, model, tokenizer):\n",
    "    input_text = f\"Query: {query}\\n\\nReport: {report_texts}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot response:\n",
      "Fast Moving Consumer Goods product sell quickly relatively low cost satisfy elemental day to day household need grocery range package. FMCG sector worth r 1,300 billion expect around whopping value r 4,000 r 6,000 billion 2020 henceforth fMCG close companion retail sector likely create job.\n"
     ]
    }
   ],
   "source": [
    "# Chatbot function to handle user queries related to campaign performance\n",
    "def chatbot_query_with_advanced_summary(user_input):\n",
    "    query_embedding = generate_query_embedding(user_input, fine_tuned_model, fine_tuned_tokenizer, device)\n",
    "    similar_reports_indices = find_similar_embeddings(query_embedding, loaded_embeddings)\n",
    "    similar_reports = retrieve_reports_from_indices(similar_reports_indices)\n",
    "    combined_reports_text = \" \".join(similar_reports)\n",
    "    advanced_summary = generate_query_based_summary(user_input, combined_reports_text, bart_model, bart_tokenizer)\n",
    "    return advanced_summary\n",
    "\n",
    "# Example chatbot interaction\n",
    "user_input = \"How did the recent marketing campaigns perform?\"\n",
    "chatbot_response = chatbot_query_with_advanced_summary(user_input)\n",
    "print(f\"Chatbot response:\\n{chatbot_response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
